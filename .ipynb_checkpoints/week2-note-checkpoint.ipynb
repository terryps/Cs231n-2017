{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN ##\n",
    "1. spatial pattern을 찾기 위해 쓴다.\n",
    "2. NN에 비해 parameter를 줄여준다. 3차원 데이터를 1차원으로 만들 필요없이 공간적인 구조를 유지하면서 학습이 가능하다.\n",
    "\n",
    "\n",
    "+ ((Conv-ReLU)N - POOL?)M - (FC-ReLU)*K, Softmax Classifier\n",
    "+ CNN은 layer가 많기 때문에 거의 ReLU를 쓴다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv layer ###\n",
    "+ Conv layer는 filter로 지역을 대표하는 값을 계산한다.\n",
    "+ filter의 depth는 input의 depth와 같아야한다.\n",
    "+ ex) input shape: (32, 32, 3), filter shape: (5, 5, 3), stride==1<br>after convolution: (28, 28, 1)\n",
    "+ 가중치와 data set의 작은 지역의 feature끼리 dot product를 계산한다.\n",
    "+ 복잡한 특징을 가진 이미지를 분석하기 위해 보통 필터를 여러 개 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling ###\n",
    "+ parameter 양과 계산량을 줄이고, 결과적으로 overfit을 방지한다.\n",
    "+ Max Pooling: 지역을 대표할 가장 큰 점수를 선택한다.\n",
    "+ Pooling은 써도 되고 안써도 된다. Pooling은 여러 값 중 하나를 버리는 것이기 때문에 Pooling 대신 layer를 더 쌓기도 한다.\n",
    "+ input shape: (width w, height h, dimension d)\n",
    "+ hyperparameters: spatial extent F, stride S\n",
    "+ output shape: ((W - F)/S + 1, (H - F)/S + 1, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding ###\n",
    "+ output의 spatial size를 조절해준다.\n",
    "+ Conv layer에 의해 spatial size가 너무 빨리 shrink하면 데이터 패턴을 학습하는데 어려움이 있기 때문에 크기를 유지하기 위해 쓴다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected layer ###\n",
    "+ Conv layer는 지역적 특징을 가진 parameter들을 찾아낸다.\n",
    "+ Fully Connected layer도 위의 역할을 하지만 주된 역할은 classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization ###\n",
    "+ 네트워크의 gradient flow를 향상시킨다.\n",
    "+ 모든 layer에 대해 unit gaussian을 가질 수 있다.\n",
    "+ Batch Normalization은 Fully-Connected layer와 non-linearity(activation func) 사이에 삽입된다.\n",
    "+ weight를 곱할수록 bad scaling effect를 가질 수 있는데 이를 방지한다. 예를 들어, activation이 ReLU일 경우, layer가 깊어지면 weight를 계속 곱하다가 값이 너무 커지면 backpropagation때 편미분값이 너무 커져서 Loss가 오히려 0에서 멀어진다.\n",
    "+ 이를 방지하기 위해, activation func의 input을 normalize 해준다.\n",
    "~~~\n",
    "    (x - E[x]) / std\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gamma & Beta ###\n",
    "+ non-linearity(activation)에서 어느 정도의 saturation을 허락하고 싶으면 가우시안 분포를 scaling해주고 shifting해준다. 왜냐하면 항상 unit gaussian이 optimal하지 않을 수 있기 때문이다.\n",
    "~~~\n",
    "    score = gamma * x + beta\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter updates ###\n",
    "+ parameter를 update하는 여러가지 접근법.\n",
    "+ Vanilla update: negative direction을 따라 parameter를 update한다.\n",
    "~~~\n",
    "    w += -learning_rate * dw\n",
    "~~~\n",
    "+ Momentum update: 물리적인 관점을 적용한 parameter update다. Loss를 산이 많은 지형의 높이로 본다.\n",
    "~~~\n",
    "    # rho는 마찰력, vx는 현재 지점에서의 속도\n",
    "    vx = 0\n",
    "    while True:\n",
    "        vx = rho * vx + dx\n",
    "        x += learning_rate * vx\n",
    "~~~\n",
    "<br>\n",
    "    - 공이 지형에서 구르면서 속도가 붙는다. local minima나 saddle point에 도달해도 비록 gradient는 0이지만 그 지점에서의 속도는 아직 남아있기 때문에 계속 아래로 갈 수 있을 것이다.\n",
    "    - 아래의 문제를 해결 할 수 있다.<br>1. 가중치마다 update 속도가 다르다. 값의 변화에 따라 유동성이 크게 차이난다.<br>2. loss func의 saddle point와 local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
