{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network ###\n",
    "\n",
    "+ 생물의 뇌를 구성하는 신경망을 모방한 학습 알고리즘\n",
    "+ Maximum Likelihood Estimation(MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](./img/XOR.PNG)\n",
    "+ linear classifier는 아래의 두 조건을 만족해야 한다.\n",
    "    1. f(ax)=af(x)\n",
    "    2. f(x+y)=f(x)+f(y)\n",
    "+ linear classifier로 해결 못하는 문제를 linear transformation(W)과 non-linear transformation(activation func)을 하여<br>최종적으로 linearly seperable이 가능하게 만든다.\n",
    "[colah.github.io](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)\n",
    "+ output layer가 마지막 linear classifier 역할을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 아키텍쳐를 고른다. (hidden layer)\n",
    "2. weights를 랜덤하게 초기화한다.\n",
    "3. forward pass로 output을 구한다.\n",
    "4. loss를 구한다.\n",
    "5. partial derivative를 구하기위해 backpropagation\n",
    "6. optimize\n",
    "\n",
    "3~6을 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The advantages of Relu over sigmoid function ###\n",
    "sigmoid func은 역치를 어느정도 넘기면 편미분값이 0에 가까워지기 때문에\n",
    "backpropagation을 할수록 input layer에 가까운 노드들은 update가 안된다. chain rule 때문에 앞쪽에 있는 노드는 w.assign(w+0.0000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choosing the num of hidden layers and nodes in NN ###\n",
    "+ 네트워크가 일단 초기화되면, 보조 알고리즘들을 써서 training을 시키는동안 configuration을 반복적으로 조율가능하다.\n",
    "+ 특정 횟수의 training epoch 후에 가중치 벡터의 값들이 작으면 불필요한 노드로 간주하여 pruning한다.\n",
    "+ So every NN has three types of layers: input, hidden, and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Classifier ###\n",
    "+ class에 대한 확률분포를 구하기 위해 쓴다.\n",
    "+ 예측한 class 확률분포와 label 확률 분포를 비슷하게 만드는 것이 목표다.\n",
    "+ loss function 값이 0에 가까워야한다.\n",
    "\n",
    "~~~\n",
    "  # softmax\n",
    "  # shape of X: (N, D), shape of W: (D, C), shape of n_score: (N, C)\n",
    "  score = np.dot(X, W)\n",
    "  \n",
    "  # take all scores, exponentiate them so that they become positive\n",
    "  exp_score = np.exp(score)\n",
    "  denominator = np.sum(exp_score, axis=1, keepdims=True)\n",
    "  normalized_score = exp_score / denominator\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation ###\n",
    "+ 어떤 가중치에 대한 모델의 편미분 값은 그 가중치가 전체 표현식에 미치는 영향력을 말해준다.\n",
    "+ chain rule에 의해 hidden layer의 편미분 값들을 구할 수 있다\n",
    "+ forward pass는 inputs으로부터 output을 계산한다.\n",
    "+ backward pass는 backpropagation을 수행함으로써, chain rule을 적용하여 gradients를 구하고, 가중치를 업데이트한다. 모든 가중치에 대한 loss func 편미분값이 0에 가까워지도록한다.(convex)\n",
    "+ dLoss/dw = (dLoss/dscore) * (dscore/dw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
